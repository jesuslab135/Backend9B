import os
import sys
import django
import joblib
import pandas as pd
import numpy as np
from datetime import datetime

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'WearableApi.settings')
django.setup()

from api.models import Lectura, Ventana, Analisis, Consumidor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix

def extract_features_from_lecturas():
    print("üìä Extrayendo datos de la base de datos...")
    
    lecturas = Lectura.objects.select_related('ventana').all()
    
    if lecturas.count() == 0:
        print("‚ùå No hay lecturas en la base de datos!")
        print("üí° Sugerencia: Inserta datos de prueba primero")
        return None
    
    print(f"‚úÖ Encontradas {lecturas.count()} lecturas")
    
    data = []
    for lectura in lecturas:
        data.append({
            'ventana_id': lectura.ventana_id,
            'heart_rate': lectura.heart_rate or 0,
            'accel_x': lectura.accel_x or 0,
            'accel_y': lectura.accel_y or 0,
            'accel_z': lectura.accel_z or 0,
            'gyro_x': lectura.gyro_x or 0,
            'gyro_y': lectura.gyro_y or 0,
            'gyro_z': lectura.gyro_z or 0,
        })
    
    df = pd.DataFrame(data)
    return df

def engineer_features(df):
    print("üîß Creando features adicionales...")
    
    features_per_window = []
    
    for ventana_id in df['ventana_id'].unique():
        window_data = df[df['ventana_id'] == ventana_id]
        
        features = {
            'ventana_id': ventana_id,
            
            'hr_mean': window_data['heart_rate'].mean(),
            'hr_std': window_data['heart_rate'].std(),
            'hr_min': window_data['heart_rate'].min(),
            'hr_max': window_data['heart_rate'].max(),
            'hr_range': window_data['heart_rate'].max() - window_data['heart_rate'].min(),
            
            'accel_magnitude_mean': np.sqrt(
                window_data['accel_x']**2 + 
                window_data['accel_y']**2 + 
                window_data['accel_z']**2
            ).mean(),
            'accel_magnitude_std': np.sqrt(
                window_data['accel_x']**2 + 
                window_data['accel_y']**2 + 
                window_data['accel_z']**2
            ).std(),
            
            'gyro_magnitude_mean': np.sqrt(
                window_data['gyro_x']**2 + 
                window_data['gyro_y']**2 + 
                window_data['gyro_z']**2
            ).mean(),
            'gyro_magnitude_std': np.sqrt(
                window_data['gyro_x']**2 + 
                window_data['gyro_y']**2 + 
                window_data['gyro_z']**2
            ).std(),
            
            'accel_energy': (window_data['accel_x']**2 + 
                           window_data['accel_y']**2 + 
                           window_data['accel_z']**2).sum(),
            'gyro_energy': (window_data['gyro_x']**2 + 
                          window_data['gyro_y']**2 + 
                          window_data['gyro_z']**2).sum(),
        }
        
        features_per_window.append(features)
    
    features_df = pd.DataFrame(features_per_window)
    
    features_df = features_df.fillna(0)
    
    print(f"‚úÖ Creadas {len(features_df.columns)-1} features para {len(features_df)} ventanas")
    
    return features_df

def get_labels():
    print("üè∑Ô∏è  Obteniendo labels...")
    
    analisis = Analisis.objects.all()
    
    if analisis.count() > 0:
        labels_data = []
        for a in analisis:
            labels_data.append({
                'ventana_id': a.ventana_id,
                'urge_label': a.urge_label
            })
        labels_df = pd.DataFrame(labels_data)
        print(f"‚úÖ Encontrados {len(labels_df)} labels reales")
        return labels_df
    
    print("‚ö†Ô∏è  No hay an√°lisis previos. Generando labels sint√©ticos...")
    print("üí° En producci√≥n, debes etiquetar los datos realmente")
    
    from django.db.models import Avg
    
    ventanas = Ventana.objects.all()
    if ventanas.count() == 0:
        print("‚ùå No hay ventanas en la base de datos")
        return None
    
    labels_data = []
    for ventana in ventanas:
        lecturas = Lectura.objects.filter(ventana=ventana)
        if lecturas.exists():
            hr_mean = lecturas.aggregate(hr=Avg('heart_rate'))['hr'] or 70
            urge_label = 1 if hr_mean > 90 else 0
        else:
            urge_label = 0
        
        labels_data.append({
            'ventana_id': ventana.id,
            'urge_label': urge_label
        })
    
    labels_df = pd.DataFrame(labels_data)
    print(f"‚úÖ Generados {len(labels_df)} labels sint√©ticos")
    print("‚ö†Ô∏è  Recuerda: estos son datos de prueba, no reales")
    
    return labels_df

def train_model():
    print("\n" + "="*60)
    print("üöÄ ENTRENAMIENTO DEL MODELO DE PREDICCI√ìN")
    print("="*60 + "\n")
    
    lecturas_df = extract_features_from_lecturas()
    if lecturas_df is None:
        return False
    
    features_df = engineer_features(lecturas_df)
    
    labels_df = get_labels()
    if labels_df is None:
        return False
    
    print("\nüîó Combinando features y labels...")
    data = features_df.merge(labels_df, on='ventana_id', how='inner')
    
    if len(data) == 0:
        print("‚ùå No hay datos para entrenar despu√©s del merge")
        return False
    
    print(f"‚úÖ Dataset final: {len(data)} muestras")
    
    # ‚ö†Ô∏è IMPORTANTE: Remover ventana_id para evitar data leakage
    X = data.drop(['ventana_id', 'urge_label'], axis=1)
    y = data['urge_label']
    
    print(f"\nüìä Features usados en el modelo: {list(X.columns)}")
    print(f"   Total features: {len(X.columns)}")
    
    print(f"\nüìä Distribuci√≥n de clases:")
    print(f"   - Sin deseo (0): {(y == 0).sum()} muestras ({(y == 0).mean()*100:.1f}%)")
    print(f"   - Con deseo (1): {(y == 1).sum()} muestras ({(y == 1).mean()*100:.1f}%)")
    
    print("\n‚úÇÔ∏è  Dividiendo datos en train/test (80/20)...")
    
    min_class_count = y.value_counts().min()
    use_stratify = min_class_count >= 2 and len(y.unique()) > 1
    
    if not use_stratify:
        print(f"‚ö†Ô∏è  Estratificaci√≥n desactivada (muy pocos datos en alguna clase)")
        print(f"   M√≠nimo por clase: {min_class_count} muestras")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2, 
        random_state=42,
        stratify=y if use_stratify else None
    )
    
    print(f"   - Train: {len(X_train)} muestras")
    print(f"   - Test: {len(X_test)} muestras")
    
    print("\nüîÑ Normalizando features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("\nü§ñ Eligiendo modelo...")
    print("   Opci√≥n 1: Logistic Regression (interpretable, r√°pido)")
    print("   Opci√≥n 2: Random Forest (m√°s robusto, menos overfitting)")
    
    # Probar ambos modelos
    use_random_forest = len(data) > 80  # RF para datasets m√°s grandes
    
    if use_random_forest:
        print("\nüå≤ Entrenando Random Forest...")
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,  # Limitar profundidad para evitar overfitting
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            class_weight='balanced',
            max_features='sqrt'
        )
        model.fit(X_train_scaled, y_train)
        print("‚úÖ Random Forest entrenado!")
        
        # Feature importance para Random Forest
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nüîç Top 5 features m√°s importantes:")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"   üåü {row['feature']}: {row['importance']:.4f}")
    else:
        print("\nüéì Entrenando Logistic Regression con regularizaci√≥n...")
        model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced',
            C=1.0,  # Regularizaci√≥n moderada
            penalty='l2',
            solver='lbfgs'
        )
        model.fit(X_train_scaled, y_train)
        print("‚úÖ Logistic Regression entrenado!")
        
        # Mostrar features m√°s importantes
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'coefficient': model.coef_[0]
        }).sort_values('coefficient', key=abs, ascending=False)
        
        print("\nüîç Top 5 features m√°s influyentes:")
        for idx, row in feature_importance.head(5).iterrows():
            direction = "‚Üë" if row['coefficient'] > 0 else "‚Üì"
            print(f"   {direction} {row['feature']}: {row['coefficient']:.4f}")
    
    print("\nüìà Evaluando modelo...")
    
    y_train_pred = model.predict(X_train_scaled)
    train_accuracy = accuracy_score(y_train, y_train_pred)
    
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    
    print(f"\n‚úÖ M√âTRICAS DEL MODELO:")
    print(f"   üìö Train Accuracy: {train_accuracy:.3f}")
    print(f"   üìä Test Metrics:")
    print(f"      - Accuracy:  {accuracy:.3f}")
    print(f"      - Precision: {precision:.3f}")
    print(f"      - Recall:    {recall:.3f}")
    print(f"      - F1-Score:  {f1:.3f}")
    
    # Validaci√≥n de sanity check
    if train_accuracy >= 0.99:
        print(f"\n‚ö†Ô∏è  ALERTA: Train accuracy sospechosamente alta ({train_accuracy:.3f})")
        print(f"   Posible data leakage o features con informaci√≥n del futuro")
        print(f"   Verifica que no est√©s usando IDs o timestamps como features")
    
    if train_accuracy - accuracy > 0.15:
        print(f"\n‚ö†Ô∏è  WARNING: Posible overfitting detectado!")
        print(f"   Train accuracy ({train_accuracy:.3f}) >> Test accuracy ({accuracy:.3f})")
        print(f"   Considera: m√°s datos, m√°s regularizaci√≥n, o features m√°s simples")
    elif train_accuracy - accuracy < 0.05:
        print(f"\n‚úÖ Buen balance train/test ({train_accuracy:.3f} vs {accuracy:.3f})")
        print(f"   El modelo generaliza bien")
    
    if len(y_test.unique()) > 1:
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        print(f"      - ROC-AUC:   {roc_auc:.3f}")
    
    print("\nüìä Reporte de clasificaci√≥n:")
    print(classification_report(y_test, y_pred, zero_division=0))
    
    print("\nüéØ Matriz de confusi√≥n:")
    cm = confusion_matrix(y_test, y_pred)
    print(f"   TN: {cm[0][0]:3d}  |  FP: {cm[0][1]:3d}")
    print(f"   FN: {cm[1][0]:3d}  |  TP: {cm[1][1]:3d}")
    
    if cm[0][1] > 0:
        print(f"\n‚ö†Ô∏è  {cm[0][1]} Falsos Positivos (predijo craving cuando no hab√≠a)")
    if cm[1][0] > 0:
        print(f"‚ö†Ô∏è  {cm[1][0]} Falsos Negativos (perdi√≥ {cm[1][0]} cravings reales)")
    
    os.makedirs('models', exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_path = f'models/smoking_craving_model_{timestamp}.pkl'
    
    print(f"\nüíæ Guardando modelo en: {model_path}")
    
    model_package = {
        'model': model,
        'scaler': scaler,
        'feature_names': X.columns.tolist(),
        'training_date': datetime.now().isoformat(),
        'metrics': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
    }
    
    joblib.dump(model_package, model_path)
    print("‚úÖ Modelo guardado!")
    
    latest_model_path = 'models/smoking_craving_model.pkl'
    if os.path.exists(latest_model_path):
        os.remove(latest_model_path)
    
    import shutil
    shutil.copy(model_path, latest_model_path)
    print(f"‚úÖ Symlink creado: {latest_model_path}")
    
    print("\n" + "="*60)
    print("üéâ ENTRENAMIENTO COMPLETADO EXITOSAMENTE")
    print("="*60)
    print(f"\nüìÅ Modelo guardado en: {latest_model_path}")
    print(f"üîß Ahora puedes usar Celery para hacer predicciones!")
    
    return True

def generate_synthetic_window(consumidor, pattern_type):
    """
    Helper to generate a window with MORE REALISTIC and OVERLAPPING patterns:
    - 'rest': Low HR, Low Motion (Label 0)
    - 'exercise': High HR, High Motion (Label 0)
    - 'craving': Elevated HR, Moderate Motion (Label 1) - OVERLAP with rest/exercise
    """
    from django.utils import timezone
    
    window_start = timezone.now() - timezone.timedelta(days=np.random.randint(0, 30))
    ventana = Ventana.objects.create(
        consumidor=consumidor,
        window_start=window_start,
        window_end=window_start + timezone.timedelta(minutes=5)
    )
    
    # Define base parameters with MORE OVERLAP to make it realistic
    if pattern_type == 'craving':
        # CRAVING: Slightly elevated HR, moderate motion (NOT perfect separation)
        hr_base = np.random.uniform(75, 95)  # Overlap with rest and light exercise
        motion_intensity = np.random.uniform(0.3, 0.8)  # Some motion variance
        hr_variance = 8  # Higher variance (stress/anxiety)
        urge_label = 1
    elif pattern_type == 'exercise':
        # EXERCISE: High HR and High Motion (but some overlap with craving HR)
        hr_base = np.random.uniform(95, 130)  # Can overlap with craving
        motion_intensity = np.random.uniform(1.5, 3.0)  # High motion
        hr_variance = 12  # High variance during activity
        urge_label = 0
    else: # 'rest'
        # REST: Low HR and Low Motion (but can have outliers)
        hr_base = np.random.uniform(60, 80)  # Can overlap with craving
        motion_intensity = np.random.uniform(0.1, 0.5)  # Some fidgeting
        hr_variance = 5  # Low variance at rest
        urge_label = 0
    
    # Add outlier windows (10% chance) to make it harder
    if np.random.random() < 0.1:
        hr_base += np.random.choice([-10, 10])  # Sudden change
        motion_intensity += np.random.uniform(-0.2, 0.2)
        
    for _ in range(30):
        # Add realistic fluctuation with noise
        hr = np.random.normal(hr_base, hr_variance)
        
        # Add motion noise and outliers (sensor errors)
        accel = np.abs(np.random.normal(motion_intensity, motion_intensity * 0.4))
        gyro = np.abs(np.random.normal(motion_intensity * 0.4, motion_intensity * 0.3))
        
        # 5% chance of sensor glitch
        if np.random.random() < 0.05:
            accel *= np.random.uniform(0.5, 2.0)
            gyro *= np.random.uniform(0.5, 2.0)
        
        Lectura.objects.create(
            ventana=ventana,
            heart_rate=max(50, min(180, hr)),
            accel_x=accel * np.random.randn(),
            accel_y=accel * np.random.randn(),
            accel_z=accel * np.random.randn(),
            gyro_x=gyro * np.random.randn(),
            gyro_y=gyro * np.random.randn(),
            gyro_z=gyro * np.random.randn()
        )
    
    # Create analysis label with realistic probability (not 0.1 or 0.9)
    if urge_label == 1:
        prob = np.random.uniform(0.55, 0.85)  # Not too confident
    else:
        prob = np.random.uniform(0.15, 0.45)  # Some uncertainty
        
    Analisis.objects.create(
        ventana=ventana,
        probabilidad_modelo=prob,
        urge_label=urge_label,
        modelo_usado='realistic_synthetic_data'
    )
    return ventana

def insert_sample_data():
    print("\nüîß ¬øQuieres insertar datos de muestra MEJORADOS? (y/n): ", end='')
    response = input().strip().lower()
    
    if response != 'y':
        return
    
    print("\nüìù Insertando datos de entrenamiento inteligentes...")
    
    try:
        from api.models import Usuario
        # Try to find a consumer, or use test user
        usuario = Usuario.objects.filter(consumidor__isnull=False).first()
        if not usuario:
            print("‚ùå No hay usuarios consumidor en la BD")
            return
        consumidor = usuario.consumidor
    except:
        print("‚ùå Error buscando usuario")
        return
    
    print("üì¶ Generando 100 ventanas con patrones realistas y overlapping...")
    print("   - 40 Reposo (Label 0) - Puede tener HR elevado ocasionalmente")
    print("   - 30 Ejercicio (Label 0) - Puede tener momentos de calma")
    print("   - 30 Ansiedad/Deseo (Label 1) - Overlaps con reposo/ejercicio")
    
    patterns = []
    patterns.extend(['rest'] * 40)
    patterns.extend(['exercise'] * 30)
    patterns.extend(['craving'] * 30)
    
    # Shuffle para evitar orden artificial
    np.random.shuffle(patterns)
    
    for i, pattern in enumerate(patterns):
        generate_synthetic_window(consumidor, pattern)
        
        if (i+1) % 20 == 0:
            print(f"   ‚úÖ {i+1}/100 ventanas creadas...")
            
    print(f"‚úÖ 100 ventanas realistas insertadas con overlap")

def insert_sample_data_auto():
    print("\nüìù Insertando datos autom√°ticos realistas (Modo CI/CD)...")
    try:
        from api.models import Usuario
        usuario = Usuario.objects.filter(consumidor__isnull=False).first()
        if not usuario:
            return
        consumidor = usuario.consumidor
        
        # Generate realistic overlapping dataset
        patterns = ['rest'] * 40 + ['exercise'] * 30 + ['craving'] * 30
        np.random.shuffle(patterns)
        
        for i, pattern in enumerate(patterns):
            generate_synthetic_window(consumidor, pattern)
            if (i+1) % 25 == 0:
                print(f"   ‚úÖ {i+1}/100 ventanas generadas...")
            
        print(f"‚úÖ 100 ventanas con patrones overlapping generadas")
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    import sys
    
    print("\n" + "="*60)
    print("ü§ñ SISTEMA DE ENTRENAMIENTO DE MODELO ML")
    print("="*60)
    
    from api.models import Lectura
    if Lectura.objects.count() == 0:
        print("\n‚ö†Ô∏è  No hay datos en la tabla 'lecturas'")
        
        if '--auto' in sys.argv or '-y' in sys.argv:
            print("üîß Insertando datos autom√°ticamente (flag --auto detectado)...")
            insert_sample_data_auto()
        else:
            insert_sample_data()
    
    success = train_model()
    
    if not success:
        print("\n‚ùå El entrenamiento fall√≥")
        sys.exit(1)
    
    print("\n‚úÖ Todo listo para usar el sistema de predicci√≥n!")

def insert_sample_data_auto():
    print("\nüìù Insertando datos de muestra m√°s realistas...")
    
    from django.utils import timezone
    from django.db import models
    
    try:
        from api.models import Usuario
        usuario = Usuario.objects.filter(consumidor__isnull=False).first()
        if not usuario:
            print("‚ùå No hay usuarios consumidor en la BD")
            return
        consumidor = usuario.consumidor
        print(f"‚úÖ Usando consumidor: {usuario.email}")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return
    
    print("üì¶ Creando 50 ventanas con patrones variados...")
    
    for i in range(50):
        ventana = Ventana.objects.create(
            consumidor=consumidor,
            window_start=timezone.now(),
            window_end=timezone.now() + timezone.timedelta(minutes=5)
        )
        
        urge_probability = np.random.random()
        is_high_urge = urge_probability > 0.7
        
        for j in range(30):
            if is_high_urge:
                hr_base = np.random.uniform(85, 105)
                hr = np.random.normal(hr_base, 12)
                accel = np.random.normal(1.3, 0.6)
                gyro = np.random.normal(0.7, 0.35)
            else:
                hr_base = np.random.uniform(60, 80)
                hr = np.random.normal(hr_base, 8)
                accel = np.random.normal(0.6, 0.3)
                gyro = np.random.normal(0.25, 0.15)
            
            if np.random.random() < 0.15:
                hr += np.random.uniform(-15, 15)
                accel += np.random.uniform(-0.4, 0.4)
                gyro += np.random.uniform(-0.2, 0.2)
            
            Lectura.objects.create(
                ventana=ventana,
                heart_rate=max(50, min(150, hr)),
                accel_x=accel * np.random.randn(),
                accel_y=accel * np.random.randn(),
                accel_z=accel * np.random.randn(),
                gyro_x=gyro * np.random.randn(),
                gyro_y=gyro * np.random.randn(),
                gyro_z=gyro * np.random.randn()
            )
        
        if is_high_urge:
            prob = np.random.uniform(0.65, 0.95)
        else:
            prob = np.random.uniform(0.05, 0.35)
        
        Analisis.objects.create(
            ventana=ventana,
            probabilidad_modelo=prob,
            urge_label=1 if is_high_urge else 0,
            modelo_usado='manual_label'
        )
        
        if (i + 1) % 10 == 0:
            print(f"   ‚úÖ {i + 1}/50 ventanas creadas...")
    
    print(f"‚úÖ Insertadas 50 ventanas con 1500 lecturas")
    print(f"‚úÖ Insertados 50 an√°lisis para labels")
    print(f"üí° Datos m√°s realistas con overlap y ruido")

